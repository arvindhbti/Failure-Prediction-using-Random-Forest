import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import recall_score
from sklearn.metrics import roc_curve, auc, roc_auc_score, average_precision_score
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE
from sklearn.utils import resample
from operator import itemgetter
%matplotlib inline



data_bmcc1=pd.read_csv("C:/Users/Arvind/Desktop/failure prediction/EM-data/EM-data/BMCC1_final_data.csv")
data_bmcc1_filter=data_bmcc1[['V1','V2','V3','L1','L2','L3','PF1','PF2','PF3','FREQUENCY','failed']]
X=data_bmcc1_filter[['V1','V2','V3','L1','L2','L3','PF1','PF2','PF3','FREQUENCY']]
y=data_bmcc1_filter['failed']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)


def evaluate_model_auc(model, X_test_parameter, y_test_parameter):
    y_pred = model.predict(X_test_parameter)
    fp_rate, tp_rate, treshold = roc_curve(y_test_parameter, y_pred)
    auc_score = auc(fp_rate, tp_rate)
    return (auc_score)

def plot_auc(model, X_test, y_test):
    y_pred = model.predict(X_test)
    
    fp_rate, tp_rate, treshold = roc_curve(y_test, y_pred)
    auc_score = auc(fp_rate, tp_rate)
    
    plt.figure()
    plt.title('ROC Curve')
    plt.plot(fp_rate, tp_rate, 'b', label = 'AUC = %0.2f' % auc_score)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

def evaluate_model_score(model, X_test, y_test):
    return model.score(X_test, y_test)


def evaluate_classification_report(model, y_test):
    return classification_report(y_test, model.predict(X_test), target_names=['Not Failed', 'Failed'])


def evaluate_model(model_param, X_test_param, y_test_param):
    print("Model evaluation")
    print("Accuracy: {:.5f}".format(evaluate_model_score(model_param, X_test_param, y_test_param)))
    print("AUC: {:.5f}".format(evaluate_model_auc(model_param, X_test_param, y_test_param)))
    print("\n#### Classification Report ####\n")
    print(evaluate_classification_report(model_param, y_test_param))
    plot_auc(model_param, X_test_param, y_test_param)


def gridsearch_results(gridsearch_model):
    print('Best score: {} '.format(gridsearch_model.best_score_))
    print('\n#### Best params ####\n')
    print(gridsearch_model.best_params_)


    
def model_selection(min_estimator, max_estimator, X_train_param, y_train_param,
                   X_test_param, y_test_param, scoring='accuracy'):
    scores = [] 
    if (scoring == 'accuracy'):
        for n in range(min_estimator, max_estimator):
            rfc_selection = RandomForestClassifier(n_estimators=n, random_state=42).fit(X_train_param, y_train_param)
            score = evaluate_model_score(rfc_selection, X_test_param, y_test_param)
            print('Number of estimators: {} - Score: {:.5f}'.format(n, score))
            scores.append((rfc_selection, score))
            
    elif (scoring == 'auc'):
         for n in range(min_estimator, max_estimator):
            rfc_selection = RandomForestClassifier(n_estimators=n, random_state=42).fit(X_train_param, y_train_param)
            score = evaluate_model_auc(rfc_selection, X_test_param, y_test_param)
            print('Number of estimators: {} - AUC: {:.5f}'.format(n, score))
            scores.append((rfc_selection, score))
    return sorted(scores, key=lambda x: x[1], reverse=True)[0][0]


sm = SMOTE(random_state=2)
X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())


model_rfc = RandomForestClassifier().fit(X_train, y_train)
evaluate_model(model_rfc, X_test, y_test)
evaluate_classification_report(model_rfc,y_test)

rfc_model = model_selection(5, 15, X_train, y_train, X_test, y_test, scoring='auc')
evaluate_model(rfc_model, X_test, y_test)

#smote
rfc_smote = model_selection(5, 15, X_train_res, y_train_res,X_test, y_test, scoring='auc')
evaluate_model(rfc_smote, X_test, y_test)
sorted(rfc_smote.feature_importances_, reverse=True)[:5]

features = [i for i in X.columns.values]
importance = [float(i) for i in rfc_smote.feature_importances_]
feature_importance = []


for item in range(0, len(features)):
    feature_importance.append((features[item], importance[item]))

feature_importance.sort(key=itemgetter(1), reverse=True)

feature_importance[:15]



parameters_rfc = { 
    'n_estimators': [5, 6, 7, 8, 9, 10, 13, 15],
    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40],
    'min_samples_leaf': [1, 2, 3, 4, 5]
}
'''
rfc_grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42
                                                               ),
                               param_grid=parameters_rfc,
                               cv=10, 
                               scoring='roc_auc',
                               return_train_score=True)

rfc_grid_search.fit(X_train, y_train)
cv_results = pd.DataFrame(rfc_grid_search.cv_results_)
cv_results.sort_values(by='rank_test_score').head()
gridsearch_results(rfc_grid_search)
'''

rfc = RandomForestClassifier(random_state=42, n_estimators=7, min_samples_leaf=1, max_depth=5)
rfc.fit(X_train_res, y_train_res)
evaluate_model(rfc, X_test, y_test)
#evaluate_classification_report(rfc,y_test)

'''
rfc_grid_search_balanced = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                                        param_grid=parameters_rfc,cv=10,scoring='roc_auc',
                                        return_train_score=True)

'''

rfc_balanced = RandomForestClassifier(random_state=42,n_estimators=13, min_samples_leaf=4, max_depth=None)
rfc_balanced.fit(X_train_res, y_train_res)
evaluate_model(rfc_balanced, X_test, y_test)
evaluate_classification_report(rfc_balanced,y_test)
